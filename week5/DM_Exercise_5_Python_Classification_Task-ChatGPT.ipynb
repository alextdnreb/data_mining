{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below if you are using Google Colab to mount your Google Drive in your Colab instance. Adjust the path to the files in your Google Drive as needed if it differs.\n",
    "\n",
    "If you do not use Google Colab, running the cell will simply do nothing, so do not worry about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    %cd 'drive/My Drive/Colab Notebooks/05_Classification'\n",
    "except ImportError as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 5: Classification\n",
    "\n",
    "### 5.1. Parameter optimization\n",
    "In Exercise 4.2 we have used the German credit data set from the UCI data set library (http://archive.ics.uci.edu/ml/index.html), which describes the customers of a bank with respect to whether they should get a bank credit or not. The data set is provided as credit-g.arff file in ILIAS.\n",
    "\n",
    "#### 5.1.1.\t(recap) Go back to the results of exercise 4.2.4. Re-run the classifiers with their default parameter settings.\n",
    "- Used the 10-fold validation approach.\n",
    "- Balanced the training set multiplying the “bad customer” examples. \n",
    "- Evaluated the results, setting up your cost matrix to ((0,100)(1,0)) – that is, you assumed you will lose 1 Unit if you refuse a credit to a good customer, but that you lose 100 Units if you give a bad customer a credit.\n",
    "\n",
    "Rerun your process to get the performance results. What were the default parameters of the Decision Tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# load the dataset\n",
    "credit_arff_data, credit_arff_meta = arff.loadarff(open('credit-g.arff', 'r'))\n",
    "credit_data = pd.DataFrame(credit_arff_data)\n",
    "\n",
    "# select all columns of type object\n",
    "columns_with_binary_strings = credit_data.select_dtypes('object').columns.values\n",
    "\n",
    "# decode the values of these columns using utf-8\n",
    "credit_data[columns_with_binary_strings] = credit_data[columns_with_binary_strings].apply(lambda x: x.str.decode(\"utf-8\"))\n",
    "credit_target = credit_data['class']\n",
    "credit_data = credit_data.drop(columns='class')\n",
    "\n",
    "label = LabelEncoder()\n",
    "credit_target = label.fit_transform(credit_target)\n",
    "label_names=['bad','good']\n",
    "label_order=label.transform(label_names)\n",
    "\n",
    "credit_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we set up a pipeline and evaluate it using cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "numeric_features = ['duration', 'credit_amount', 'installment_commitment', 'residence_since', 'age', 'existing_credits', 'num_dependents']\n",
    "categorical_features = ['credit_history', 'purpose', 'personal_status', 'other_parties', 'property_magnitude', 'other_payment_plans', 'housing', 'job', 'own_telephone', 'foreign_worker']\n",
    "ordinal_features = [ 'checking_status', 'savings_status', 'employment']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features),\n",
    "        ('ord', OrdinalEncoder(categories=[\n",
    "            [ 'no checking', '<0', '0<=X<200', '>=200' ],\n",
    "            [ 'no known savings', '<100', '100<=X<500', '500<=X<1000', '>=1000' ],\n",
    "            [ 'unemployed', '<1', '1<=X<4', '4<=X<7', '>=7' ]\n",
    "        ]), ordinal_features)])\n",
    "\n",
    "# define a pipeline that includes a preprocessing step, an oversampling balancing step and a decision tree classifier\n",
    "#TODO: INSERT YOUR CODE HERE!\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# get the predictions from the pipeline using the created StratifiedKFold instance\n",
    "#TODO: INSERT YOUR CODE HERE!\n",
    "\n",
    "def cost_function(y_true, y_pred): \n",
    "    cm = confusion_matrix(y_true, y_pred, labels=label_order)\n",
    "    return cm[0][1] * 100 + cm[1][0] * 1\n",
    "\n",
    "# create the confusion matrix, calculate the cost with the cost_function and calculate the accuracy\n",
    "#TODO: INSERT YOUR CODE HERE!\n",
    "\n",
    "print(\"Decision Tree with accuracy of {} and cost {}\".format(acc, cost))\n",
    "plot_confusion_matrix(cm, classes=label_names, title='Decision Tree Classifier')\n",
    "plt.show()\n",
    "print(classification_report(credit_target, prediction, target_names=label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# fit the pipeline to the dataset\n",
    "pipeline.fit(credit_data, credit_target)\n",
    "\n",
    "estimator = pipeline.named_steps['estimator']\n",
    "pre = pipeline.named_steps['preprocessing']\n",
    "feature_names = numeric_features + list(pre.named_transformers_['cat'].get_feature_names(categorical_features)) + ordinal_features\n",
    "\n",
    "# plot the decision tree\n",
    "plt.figure(figsize=(20,10))\n",
    "tree.plot_tree(estimator,\n",
    "               feature_names=feature_names,\n",
    "                class_names=label_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the default parameters of the decision tree\n",
    "tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 5.1.2.\tNow try to find a more optimal configuration for the Decision Tree. What is the best parameter setting and what is the cross-validated performance of this appraoch?\n",
    "\n",
    "Use the GridSearchCV from scikit learn. Try the following parameters of the Decision Tree:\n",
    "- criterion: ['gini', 'entropy']\n",
    "- 'max_depth': [2, 3, 4, 5, None]\n",
    "- 'min_samples_split' :[2,3,4,5]\n",
    "\n",
    "What is the best configuration for the data set and the classification approach? \n",
    "\n",
    "Note: The grid search can take some time. You can use the ```n_jobs=-1``` parameter setting for the ```cross_val_predict()``` function to enable parallel processing (all CPU cores will be used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define the parameter grid\n",
    "#TODO: INSERT YOUR CODE HERE\n",
    "\n",
    "# define the folds for the cross validation\n",
    "stratified_10_fold_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# create a scorer for the grid search\n",
    "cost_score = make_scorer(cost_function, greater_is_better=False)\n",
    "\n",
    "# create the grid search estimator\n",
    "#TODO: INSERT YOUR CODE HERE\n",
    "#grid_search_estimator = ...\n",
    "\n",
    "# cross-validate\n",
    "#TODO: INSERT YOUR CODE HERE\n",
    "#prediction = ...\n",
    "\n",
    "# calculate costs\n",
    "cm = confusion_matrix(credit_target, prediction, labels=label_order)\n",
    "cost = cost_function(credit_target, prediction)\n",
    "acc = accuracy_score(credit_target, prediction)\n",
    "\n",
    "print(\"Optimised Decision Tree with accuracy of {} and cost {}\".format(acc, cost))\n",
    "plot_confusion_matrix(cm, classes=label_names, title='Decision Tree Classifier')\n",
    "plt.show()\n",
    "print(classification_report(credit_target, prediction, target_names=label_names))\n",
    "\n",
    "# fit the grid search (= determine the optimal parameters)\n",
    "#TODO: INSERT YOUR CODE HERE\n",
    "print(\"Optimised Parameters: {}\".format(grid_search_estimator.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 5.1.4.\tHow does the optimal decision tree differ from the one you have learned in 4.2.4?\n",
    "Plot the optimised tree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = grid_search_estimator.best_estimator_.named_steps['estimator']\n",
    "pre = grid_search_estimator.best_estimator_.named_steps['preprocessing']\n",
    "feature_names = numeric_features + list(pre.named_transformers_['cat'].get_feature_names(categorical_features)) + ordinal_features\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "tree.plot_tree(estimator,\n",
    "               feature_names=feature_names,\n",
    "                class_names=label_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 5.2. Open Competition: Finding rich Americans\n",
    "The Adult data set from the UCI data set library (http://archive.ics.uci.edu/ml/datasets/Adult) describes 48842 persons from the 1994 US Census. The data set is provided as adult.arff file on the website of this course. \n",
    "\n",
    "Your task is to find a good classifier for determining whether a person earns over 50.000 $ \n",
    "a year. Beside of being accurate, your classifier should also have balanced precision and recall.\n",
    "\n",
    "To evaluate your classifiers use train_test_split validation (test_size=0.2, random_state=42).\n",
    "In order to find the best classifier, you may experiment with:\n",
    "1.\tdifferent algorithms\n",
    "2.\tdifferent parameter settings\n",
    "3.\tthe balance of the two classes in the data set\n",
    "4.\tthe set of attributes that are used or not used\n",
    "5.\tother preprocessing techniques\n",
    "\n",
    "In order to increase your understanding of the data set, you might want to visualize different attributes or attribute combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "adult_arff_data, adult_arff_meta = arff.loadarff(open('adult.arff', 'r'))\n",
    "adult = pd.DataFrame(adult_arff_data)\n",
    "adult = adult.applymap(lambda x: x.decode('utf8').replace(\"'\", \"\") if hasattr(x, 'decode') else x)\n",
    "\n",
    "adult_target = np.array(adult['class'])\n",
    "adult_data = adult.drop('class', axis=1)\n",
    "adult_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Create train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "adult_data_train, adult_data_test, adult_target_train, adult_target_test = train_test_split(\n",
    "    adult_data, adult_target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a Column Transformer to handle preprocessing\n",
    "# TODO: INSERT YOUR CODE HERE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test some classification methods as baseline\n",
    "# TODO: INSERT YOUR CODE HERE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the class distribution\n",
    "class_dist = pd.Series(adult_target_train).value_counts()\n",
    "plt.bar(class_dist.index, class_dist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try balancing the dataset: add an additional step to the pipeline\n",
    "# TODO: INSERT YOUR CODE HERE!\n",
    "\n",
    "# create a grid search instance and find the optimal parameters for a decision tree classifier\n",
    "# TODO: INSERT YOUR CODE HERE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the optimal parameters for a KNN classifier\n",
    "# TODO: INSERT YOUR CODE HERE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the best estimator and the optimal hyperparameters from the above results and evaluate on the test(unseen) data\n",
    "# TODO: INSERT YOUR CODE HERE!\n",
    "\n",
    "# plot the confusion matrix and print the classification report\n",
    "cm = confusion_matrix(adult_target_test, prediction)\n",
    "plot_confusion_matrix(cm, classes=unique_labels(adult_target_test), title=\"Results on Test Set\")\n",
    "print(classification_report(adult_target_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT Bonus Exercises\n",
    "Reminder: Do not take the answers of ChatGPT at face value! Always cross-check with lecture slides, literature and/or the teaching staff!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.1. Discuss about Hyperparameter Optimization and (Nested) Cross Validation\n",
    "* Use ChatGPT to discuss what hyperparameter optimization is and why it is important for achieving good results. Can hyperparameter optimization help with the problem of overfitting? Ask ChatGPT to give you an example of overcoming overfitting in Decision Trees by performing hyperparameter optimization.\n",
    "* Discuss with ChatGPT what the difference is between nested cross validation and cross validation. What are the factors that determine the number of folds to use for nested cross validation? Is it better to use a higher number of folds?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.2. Learn about Naïve Bayes and Feature Selection\n",
    "* Ask ChatGPT how Naïve Bayes is used for classification, its advantages and disadvantages and how you can overcome these disadvantages. Ask ChatGPT to give you a small dataset and an unseen example for a pen and paper exercise on how Naïve Bayes is used to classify the unseen example. Afterwards, ask ChatGPT for Python code to load the proposed dataset and classify the unseen example using Naïve Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ask ChatGPT to list some feature selection methods and to give you an explanation for each of the methods. Afterwards, ask ChatGPT to give you Python code for performing feature selection on the “Iris” dataset. Afterwards, ask ChatGPT to give you Python code for testing the performance of Naïve Bayes on the original as well as on the reduced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.3. Self-Assessment\n",
    "* Ask ChatGPT to give you some multiple-choice questions for graduate students about Support Vector Machines and Naïve Bayes and solve the questions. Request the correct answers and compare with your own answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
